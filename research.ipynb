{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- https://towardsdatascience.com/introduction-to-latent-matrix-factorization-recommender-systems-8dfc63b94875\n",
    "- https://eugeneyan.com/writing/recommender-systems-baseline-pytorch/\n",
    "- julian mccauley amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a “pretty print” function pprint for dict objects and dataframes.\n",
    "pprint = lambda x: print(json.dumps(x, indent=2)) if isinstance(x, dict) else display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "items = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/Software.jsonl.gz\"\n",
    "\n",
    "with gzip.open(file, 'rt') as fp:\n",
    "    for line in fp:\n",
    "        reviews.append(json.loads(line.strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/meta_Software.jsonl.gz\"\n",
    "\n",
    "with gzip.open(file, 'rt') as fp:\n",
    "    for line in fp:\n",
    "        items.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_json(\"data/Software.jsonl.gz\", lines=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.read_json(\"data/meta_Software.jsonl.gz\", lines=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display basic summary stats of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## DATA INFO ############################## \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4880181 entries, 0 to 4880180\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Dtype         \n",
      "---  ------             -----         \n",
      " 0   rating             int64         \n",
      " 1   title              object        \n",
      " 2   text               object        \n",
      " 3   images             object        \n",
      " 4   asin               object        \n",
      " 5   parent_asin        object        \n",
      " 6   user_id            object        \n",
      " 7   timestamp          datetime64[ns]\n",
      " 8   helpful_vote       int64         \n",
      " 9   verified_purchase  bool          \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), object(6)\n",
      "memory usage: 339.7+ MB\n",
      "None\n",
      "############################## DATA DESCRIBE ############################## \n",
      "             rating                      timestamp  helpful_vote\n",
      "count  4.880181e+06                        4880181  4.880181e+06\n",
      "mean   3.935087e+00  2016-08-11 20:56:15.320964864  4.921711e+00\n",
      "min    1.000000e+00            1999-03-15 04:02:39 -1.000000e+00\n",
      "25%    3.000000e+00            2014-09-03 01:27:03  0.000000e+00\n",
      "50%    5.000000e+00            2016-03-19 18:49:49  0.000000e+00\n",
      "75%    5.000000e+00  2018-08-15 19:51:14.231000064  2.000000e+00\n",
      "max    5.000000e+00     2023-09-11 02:13:11.515000  1.026700e+04\n",
      "std    1.451875e+00                            NaN  3.516860e+01\n",
      "############################## DATA HEAD ############################## \n",
      "   rating                                     title  \\\n",
      "0       1                                   malware   \n",
      "1       5                               Lots of Fun   \n",
      "2       5                         Light Up The Dark   \n",
      "3       4                                  Fun game   \n",
      "4       4  I am not that good at it but my kids are   \n",
      "\n",
      "                                                text images        asin  \\\n",
      "0                                 mcaffee IS malware     []  B07BFS3G7P   \n",
      "1  I love playing tapped out because it is fun to...     []  B00CTQ6SIG   \n",
      "2  I love this flashlight app!  It really illumin...     []  B0066WJLU6   \n",
      "3                           One of my favorite games     []  B00KCYMAWK   \n",
      "4  Cute game. I am not that good at it but my kid...     []  B00P1RK566   \n",
      "\n",
      "  parent_asin                       user_id               timestamp  \\\n",
      "0  B0BQSK9QCF  AGCI7FAH4GL5FI65HYLKWTMFZ2CQ 2019-07-03 19:37:12.076   \n",
      "1  B00CTQ6SIG  AHSPLDNW5OOUK2PLH7GXLACFBZNQ 2015-02-16 20:58:56.000   \n",
      "2  B0066WJLU6  AHSPLDNW5OOUK2PLH7GXLACFBZNQ 2013-03-04 12:14:27.000   \n",
      "3  B00KCYMAWK  AH6CATODIVPVUOJEWHRSRCSKAOHA 2019-06-20 20:10:28.662   \n",
      "4  B00P1RK566  AEINY4XOINMMJCK5GZ3M6MMHBN6A 2014-12-11 00:19:56.000   \n",
      "\n",
      "   helpful_vote  verified_purchase  \n",
      "0             0              False  \n",
      "1             0               True  \n",
      "2             0               True  \n",
      "3             0               True  \n",
      "4             0               True  \n",
      "############################## DATA SHAPE ############################## \n",
      "(4880181, 10)\n",
      "############################## DATA INFO ############################## \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89251 entries, 0 to 89250\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   main_category    87482 non-null  object \n",
      " 1   title            89251 non-null  object \n",
      " 2   average_rating   89226 non-null  float64\n",
      " 3   rating_number    86292 non-null  float64\n",
      " 4   features         89251 non-null  object \n",
      " 5   description      89251 non-null  object \n",
      " 6   price            70985 non-null  float64\n",
      " 7   images           89251 non-null  object \n",
      " 8   videos           89251 non-null  object \n",
      " 9   store            89038 non-null  object \n",
      " 10  categories       89251 non-null  object \n",
      " 11  details          89251 non-null  object \n",
      " 12  parent_asin      89251 non-null  object \n",
      " 13  bought_together  0 non-null      float64\n",
      " 14  subtitle         0 non-null      float64\n",
      " 15  author           0 non-null      float64\n",
      "dtypes: float64(6), object(10)\n",
      "memory usage: 10.9+ MB\n",
      "None\n",
      "############################## DATA DESCRIBE ############################## \n",
      "       average_rating  rating_number         price  bought_together  subtitle  \\\n",
      "count    89226.000000   8.629200e+04  70985.000000              0.0       0.0   \n",
      "mean         3.355644   6.269620e+02      2.956724              NaN       NaN   \n",
      "std          0.813617   1.179148e+04     25.546929              NaN       NaN   \n",
      "min          1.000000   1.000000e+00      0.000000              NaN       NaN   \n",
      "25%          3.000000   4.000000e+00      0.000000              NaN       NaN   \n",
      "50%          3.400000   1.200000e+01      0.000000              NaN       NaN   \n",
      "75%          3.800000   5.400000e+01      0.000000              NaN       NaN   \n",
      "max          5.000000   1.898759e+06   1998.000000              NaN       NaN   \n",
      "\n",
      "       author  \n",
      "count     0.0  \n",
      "mean      NaN  \n",
      "std       NaN  \n",
      "min       NaN  \n",
      "25%       NaN  \n",
      "50%       NaN  \n",
      "75%       NaN  \n",
      "max       NaN  \n",
      "############################## DATA HEAD ############################## \n",
      "          main_category                                              title  \\\n",
      "0  Appstore for Android                                 Accupressure Guide   \n",
      "1  Appstore for Android  Ankylosaurus Fights Back - Smithsonian's Prehi...   \n",
      "2  Appstore for Android                                       Mahjong 2015   \n",
      "3  Appstore for Android                              Jewels Brick Breakout   \n",
      "4  Appstore for Android                       Traffic Police: Off-Road Cub   \n",
      "\n",
      "   average_rating  rating_number  \\\n",
      "0             3.6            NaN   \n",
      "1             4.0            NaN   \n",
      "2             3.1            NaN   \n",
      "3             4.2            NaN   \n",
      "4             3.3            NaN   \n",
      "\n",
      "                                            features  \\\n",
      "0  [All the pressing point has been explained wit...   \n",
      "1  [ENCOURAGE literacy skills with highlighted na...   \n",
      "2  [Mahjong 2015 is a free solitaire matching gam...   \n",
      "3  [Game Features:, - Intuitive touch controls wi...   \n",
      "4  [In this game you will find:, - Killer police ...   \n",
      "\n",
      "                                         description  price  \\\n",
      "0  [Acupressure technique is very ancient and ver...   0.00   \n",
      "1  [Join Ankylosaurus in this interactive book ap...   2.99   \n",
      "2  [Mahjong 2015 is a free solitaire matching gam...   0.00   \n",
      "3  [Jewels Brick Breakout is a glowing jewels bri...   0.00   \n",
      "4  [Become the best road police officer in Cube C...   0.00   \n",
      "\n",
      "                                              images  \\\n",
      "0  [{'large': 'https://m.media-amazon.com/images/...   \n",
      "1  [{'large': 'https://m.media-amazon.com/images/...   \n",
      "2  [{'large': 'https://m.media-amazon.com/images/...   \n",
      "3  [{'large': 'https://m.media-amazon.com/images/...   \n",
      "4  [{'large': 'https://m.media-amazon.com/images/...   \n",
      "\n",
      "                                      videos                  store  \\\n",
      "0  [{'title': '', 'url': '', 'user_id': ''}]              mAppsguru   \n",
      "1  [{'title': '', 'url': '', 'user_id': ''}]  Oceanhouse Media, Inc   \n",
      "2  [{'title': '', 'url': '', 'user_id': ''}]            sophiathach   \n",
      "3  [{'title': '', 'url': '', 'user_id': ''}]            Bad Chicken   \n",
      "4  [{'title': '', 'url': '', 'user_id': ''}]       Dast 2 For Metro   \n",
      "\n",
      "  categories                                            details parent_asin  \\\n",
      "0         []  {'Release Date': '2015', 'Date first listed on...  B00VRPSGEO   \n",
      "1         []  {'Release Date': '2014', 'Date first listed on...  B00NWQXXHQ   \n",
      "2         []  {'Release Date': '2014', 'Date first listed on...  B00RFKP6AC   \n",
      "3         []  {'Release Date': '2015', 'Date first listed on...  B00SP2QU0E   \n",
      "4         []  {'Release Date': '2016', 'Date first listed on...  B01DZIT64O   \n",
      "\n",
      "   bought_together  subtitle  author  \n",
      "0              NaN       NaN     NaN  \n",
      "1              NaN       NaN     NaN  \n",
      "2              NaN       NaN     NaN  \n",
      "3              NaN       NaN     NaN  \n",
      "4              NaN       NaN     NaN  \n",
      "############################## DATA SHAPE ############################## \n",
      "(89251, 16)\n"
     ]
    }
   ],
   "source": [
    "# reviews summary stats\n",
    "print(\"############################## DATA INFO ############################## \")\n",
    "print(reviews_df.info())      # DataFrame info\n",
    "\n",
    "print(\"############################## DATA DESCRIBE ############################## \")\n",
    "print(reviews_df.describe())  # Descriptive statistics\n",
    "\n",
    "print(\"############################## DATA HEAD ############################## \")\n",
    "print(reviews_df.head())      # First 5 rows\n",
    "\n",
    "print(\"############################## DATA SHAPE ############################## \")\n",
    "print(reviews_df.shape)       # Shape of DataFrame\n",
    "\n",
    "# items summary stats\n",
    "print(\"############################## DATA INFO ############################## \")\n",
    "print(items_df.info())        # DataFrame info\n",
    "\n",
    "print(\"############################## DATA DESCRIBE ############################## \")\n",
    "print(items_df.describe())    # Descriptive statistics\n",
    "\n",
    "print(\"############################## DATA HEAD ############################## \")\n",
    "print(items_df.head())        # First 5 rows\n",
    "\n",
    "print(\"############################## DATA SHAPE ############################## \")\n",
    "print(items_df.shape)         # Shape of DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## NaNs ######## \n",
      "Items: \n",
      "main_category           0\n",
      "title                   0\n",
      "average_rating          0\n",
      "rating_number           0\n",
      "features                0\n",
      "description             0\n",
      "price               94886\n",
      "images                  0\n",
      "videos                  0\n",
      "store               11331\n",
      "categories              0\n",
      "details                 0\n",
      "parent_asin             0\n",
      "bought_together    112590\n",
      "dtype: int64\n",
      "\n",
      "Reviews: \n",
      "rating               0\n",
      "title                0\n",
      "text                 0\n",
      "images               0\n",
      "asin                 0\n",
      "parent_asin          0\n",
      "user_id              0\n",
      "timestamp            0\n",
      "helpful_vote         0\n",
      "verified_purchase    0\n",
      "dtype: int64\n",
      "######## Duplicates ########\n",
      "7276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "701528"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"######## NaNs ######## \")\n",
    "print(\"Items: \")\n",
    "print(items_df.isna().sum())\n",
    "print(\"\\nReviews: \")\n",
    "print(reviews_df.isna().sum())\n",
    "\n",
    "print(\"######## Duplicates ########\")\n",
    "print(reviews_df.duplicated(subset=['asin', 'user_id', 'timestamp']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-core Filtering (k=5)\n",
    "- Retain users with ≥5 reviews and items with ≥5 reviews.\n",
    "- Remove duplicate reviews, keeping the earliest for each `{userID, parent_asin}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def load_ratings(rev):\n",
    "    inters = []\n",
    "    for review in rev:\n",
    "        item, user, rating, time = review['parent_asin'], review['user_id'], review['rating'], review['timestamp']\n",
    "        inters.append((user, item, float(rating), int(time)))\n",
    "    return inters\n",
    "\n",
    "def get_user2count(inters):\n",
    "    user2count = collections.defaultdict(int)\n",
    "    for unit in inters:\n",
    "        user2count[unit[0]] += 1\n",
    "    return user2count\n",
    "\n",
    "\n",
    "def get_item2count(inters):\n",
    "    item2count = collections.defaultdict(int)\n",
    "    for unit in inters:\n",
    "        item2count[unit[1]] += 1\n",
    "    return item2count\n",
    "\n",
    "\n",
    "def generate_candidates(unit2count, threshold):\n",
    "    cans = set()\n",
    "    for unit, count in unit2count.items():\n",
    "        if count >= threshold:\n",
    "            cans.add(unit)\n",
    "    return cans, len(unit2count) - len(cans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the interactions in order and remove duplicate reviews\n",
    "\n",
    "def make_inters_in_order(inters):\n",
    "    user2inters, new_inters = collections.defaultdict(list), []\n",
    "    for inter in inters:\n",
    "        user, item, rating, timestamp = inter\n",
    "        user2inters[user].append((user, item, rating, timestamp))\n",
    "    for user in user2inters:\n",
    "        user_inters = user2inters[user]\n",
    "        user_inters.sort(key=lambda d: d[3])\n",
    "        his_items = set()\n",
    "        for inter in user_inters:\n",
    "            user, item, rating, timestamp = inter\n",
    "            if item in his_items:\n",
    "                continue\n",
    "            his_items.add(item)\n",
    "            new_inters.append(inter)\n",
    "    return new_inters\n",
    "\n",
    "# filter by k-core (5 in this case)\n",
    "def filter_inters(inters, user_k_core_threshold=0, item_k_core_threshold=0):\n",
    "    new_inters = []\n",
    "    # filter by k-core\n",
    "    if user_k_core_threshold or item_k_core_threshold:\n",
    "        print('\\nFiltering by k-core:')\n",
    "        idx = 0\n",
    "        user2count = get_user2count(inters)\n",
    "        item2count = get_item2count(inters)\n",
    "\n",
    "        while True:\n",
    "            new_user2count = collections.defaultdict(int)\n",
    "            new_item2count = collections.defaultdict(int)\n",
    "            users, n_filtered_users = generate_candidates(\n",
    "                user2count, user_k_core_threshold)\n",
    "            items, n_filtered_items = generate_candidates(\n",
    "                item2count, item_k_core_threshold)\n",
    "            if n_filtered_users == 0 and n_filtered_items == 0:\n",
    "                break\n",
    "            for unit in inters:\n",
    "                if unit[0] in users and unit[1] in items:\n",
    "                    new_inters.append(unit)\n",
    "                    new_user2count[unit[0]] += 1\n",
    "                    new_item2count[unit[1]] += 1\n",
    "            idx += 1\n",
    "            inters, new_inters = new_inters, []\n",
    "            user2count, item2count = new_user2count, new_item2count\n",
    "            print('    Epoch %d The number of inters: %d, users: %d, items: %d'\n",
    "                    % (idx, len(inters), len(user2count), len(item2count)))\n",
    "    return inters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing step\n",
    "\n",
    "def preprocess_rating(inters):\n",
    "    print('Process rating data: ')\n",
    "    print(' Dataset: reviews',)\n",
    "\n",
    "    # load ratings\n",
    "    rating_inters = load_ratings(inters)\n",
    "\n",
    "\n",
    "    # Sort and remove repeated reviews\n",
    "    rating_inters = make_inters_in_order(rating_inters)\n",
    "\n",
    "    # K-core filtering;\n",
    "    print('The number of raw inters: ', len(rating_inters))\n",
    "    kcore_rating_inters = filter_inters(rating_inters,\n",
    "                                        user_k_core_threshold=5,\n",
    "                                        item_k_core_threshold=5)\n",
    "\n",
    "    # return: list of (user_ID, item_ID, rating, timestamp)\n",
    "    return kcore_rating_inters, rating_inters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process rating data: \n",
      " Dataset: reviews\n",
      "The number of raw inters:  4828480\n",
      "\n",
      "Filtering by k-core:\n",
      "    Epoch 1 The number of inters: 1353435, users: 157062, items: 32850\n",
      "    Epoch 2 The number of inters: 1302721, users: 152319, items: 18486\n",
      "    Epoch 3 The number of inters: 1281240, users: 146980, items: 18143\n",
      "    Epoch 4 The number of inters: 1278612, users: 146779, items: 17654\n",
      "    Epoch 5 The number of inters: 1277242, users: 146453, items: 17635\n",
      "    Epoch 6 The number of inters: 1277020, users: 146436, items: 17596\n",
      "    Epoch 7 The number of inters: 1276876, users: 146402, items: 17594\n",
      "    Epoch 8 The number of inters: 1276864, users: 146399, items: 17594\n",
      "    Epoch 9 The number of inters: 1276852, users: 146399, items: 17591\n",
      "    Epoch 10 The number of inters: 1276840, users: 146396, items: 17591\n"
     ]
    }
   ],
   "source": [
    "inters,_ = preprocess_rating(reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last-out Split\n",
    "We split our dataset into training, validation, and test using the \"leave-last-out data split\" method. \n",
    "\n",
    "- Training part: the first N-2 items;\n",
    "- Validation part: the (N-1)-th item;\n",
    "- Testing part: the N-th item.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method\n",
    "\n",
    "def make_inters_in_order(inters):\n",
    "    user2inters, new_inters = collections.defaultdict(list), collections.defaultdict(list)\n",
    "    for inter in inters:\n",
    "        user, item, rating, timestamp = inter\n",
    "        user2inters[user].append((user, item, rating, timestamp))\n",
    "    for user in user2inters:\n",
    "        user_inters = user2inters[user]\n",
    "        user_inters.sort(key=lambda d: d[3])\n",
    "        his_items = set()\n",
    "        for inter in user_inters:\n",
    "            user, item, rating, timestamp = inter\n",
    "            if item in his_items:\n",
    "                continue\n",
    "            his_items.add(item)\n",
    "            new_inters[user].append(inter)\n",
    "    return new_inters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_out_split(inters):\n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    test_data = []\n",
    "\n",
    "    # Order the inters\n",
    "    ordered_inters = make_inters_in_order(inters=inters)\n",
    "\n",
    "\n",
    "    for user in tqdm(ordered_inters, desc='Creating train/valid/test lists'):\n",
    "        cur_inter = ordered_inters[user]\n",
    "        # Add the last interaction to the test set\n",
    "        test_data.append((cur_inter[-1][0], cur_inter[-1][1], cur_inter[-1][2], cur_inter[-1][3]))\n",
    "        \n",
    "        if len(cur_inter) > 1:\n",
    "            # Add the second-to-last interaction to the validation set\n",
    "            valid_data.append((cur_inter[-2][0], cur_inter[-2][1], cur_inter[-2][2], cur_inter[-2][3]))\n",
    "        \n",
    "        if len(cur_inter) > 2:\n",
    "            # Add the remaining interactions to the training set\n",
    "            for i in range(len(cur_inter) - 2):\n",
    "                train_data.append((cur_inter[i][0], cur_inter[i][1], cur_inter[i][2], cur_inter[i][3]))\n",
    "    return train_data, valid_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating train/valid/test lists: 100%|██████████| 146396/146396 [00:03<00:00, 38528.43it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = last_out_split(inters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purchasing Prediction\n",
    "In this section, I will develop several baseline models and iteratively improve upon them using advanced techniques discussed in class, as well as deep learning methods. These models will include Most Popular, item-to-item based collaborative filtering, Bayesian Personalized Ranking (BPR), and a deep learning model implemented with PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Popular Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def mostPopularClassification(data, threshold=0.75):\n",
    "    \"\"\"\n",
    "    Identifies the most popular items based on their frequency in the data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): A list of tuples or dictionaries where the first element/key represents the item.\n",
    "        threshold (float): The proportion of total occurrences to include in the popular set.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of the most popular items.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each item\n",
    "    item_counts = Counter(entry[1] for entry in data)\n",
    "    total_occurrences = sum(item_counts.values())\n",
    "\n",
    "    # Sort items by frequency in descending order\n",
    "    sorted_items = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select items until the threshold is reached\n",
    "    popular_items = set()\n",
    "    cumulative_count = 0\n",
    "    for item, count in sorted_items:\n",
    "        cumulative_count += count\n",
    "        popular_items.add(item)\n",
    "        if cumulative_count >= threshold * total_occurrences:\n",
    "            break\n",
    "\n",
    "    return popular_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_items = mostPopularClassification(train_data, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = set()\n",
    "item_set = set()\n",
    "purchased_set = set()\n",
    "\n",
    "# Collect from train and validation data\n",
    "for u, b, r,_ in train_data + valid_data:  # Avoid test_data\n",
    "    user_set.add(u)\n",
    "    item_set.add(b)\n",
    "    purchased_set.add((u, b))  # Add observed interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 984048/984048 [00:03<00:00, 323031.38it/s]\n"
     ]
    }
   ],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u,i,r,_ in tqdm(train_data):\n",
    "    ratingsPerUser[u].append((i,r))\n",
    "    ratingsPerItem[i].append((u,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make validation dataset include negative sets at an equal rate as positive set\n",
    "\n",
    "def generate_negatives(valid_data, item_set):\n",
    "    \"\"\"\n",
    "    Generate one negative sample for each positive interaction in valid_data.\n",
    "    \n",
    "    Args:\n",
    "        valid_data (list): List of tuples (user, item, ...) representing positive interactions.\n",
    "        item_set (set): Set of all possible items.\n",
    "        purchased_set (set): Set of all (user, item) pairs that are positive interactions.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of negative samples corresponding to each positive interaction.\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    for user, item, rating, time in tqdm(valid_data, desc= \"Creating negative set\"):  # Unpack the valid_data tuple\n",
    "        # Get items the user has not purchased\n",
    "        non_purchased_items = item_set - {i for i, r in ratingsPerUser[user]}\n",
    "        \n",
    "        # Randomly sample one non-purchased item\n",
    "        if non_purchased_items:\n",
    "            sampled_item = random.choice(list(non_purchased_items))\n",
    "            negatives.append((user, sampled_item, rating, time, 0))  # Label as 0 for negative sample\n",
    "    \n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating negative set: 100%|██████████| 146396/146396 [02:36<00:00, 936.67it/s] \n"
     ]
    }
   ],
   "source": [
    "neg_valid_data = generate_negatives(valid_data, item_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_valid_data = [(u, i, r, t, 1) for u, i, r, t in valid_data]\n",
    "valid_data_full = pos_valid_data + neg_valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring performance of model for Most Popular model\n",
    "def mostPopular_accuracy(valid_data):\n",
    "    accurate = 0\n",
    "    for u, i,_,_, l  in valid_data:\n",
    "        prediction = 1 if i in popular_items else 0  # Predicted label\n",
    "        if(prediction == l):\n",
    "            accurate += 1\n",
    "        return (accurate / len(valid_data))\n",
    "\n",
    "def precision_at_k(recommendations, relevant_items):\n",
    "    return len(recommendations & relevant_items) / len(recommendations)\n",
    "\n",
    "def recall_at_k(most_popular, relevant_items):\n",
    "    if len(relevant_items) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return len(set(most_popular) & set(relevant_items)) / len(relevant_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.15030674846625766\n",
      "accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Testing against validation dataset to optimize threshold\n",
    "valid_data_set = {x[1] for x in valid_data}\n",
    "\n",
    "precision = precision_at_k(popular_items, valid_data_set)\n",
    "recall = recall_at_k(popular_items, valid_data_set)\n",
    "accuracy = mostPopular_accuracy(valid_data_full)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item-to-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = set()\n",
    "item_set = set()\n",
    "purchased_set = set()\n",
    "\n",
    "# Collect from train and validation data\n",
    "for u, b, r,_ in train_data + valid_data:  # Avoid test_data\n",
    "    user_set.add(u)\n",
    "    item_set.add(b)\n",
    "    purchased_set.add((u, b))  # Add observed interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set) \n",
    "ratingDict = defaultdict(list)\n",
    "\n",
    "for u,b,r in train_data:\n",
    "    ratingsPerUser[u].append((b,r))\n",
    "    ratingsPerItem[b].append((u,r))\n",
    "    itemsPerUser[u].add(b)\n",
    "    ratingDict[(u, b)] = int(r)\n",
    "    usersPerItem[b].add(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine(i1, i2):\n",
    "    # Between two items\n",
    "    inter = usersPerItem[i1].intersection(usersPerItem[i2])\n",
    "    numer = 0\n",
    "    denom1 = 0\n",
    "    denom2 = 0\n",
    "    for u in inter:\n",
    "        numer += ratingDict[(u,i1)]*ratingDict[(u,i2)]\n",
    "    for u in usersPerItem[i1]:\n",
    "        denom1 += ratingDict[(u,i1)]**2\n",
    "    for u in usersPerItem[i2]:\n",
    "        denom2 += ratingDict[(u,i2)]**2\n",
    "    denom = math.sqrt(denom1) * math.sqrt(denom2)\n",
    "    if denom == 0: return 0\n",
    "    return numer / denom\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer/denom\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itemsPerUser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m userAverages \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m itemAverages \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m \u001b[43mitemsPerUser\u001b[49m:\n\u001b[1;32m      5\u001b[0m     rs \u001b[38;5;241m=\u001b[39m [ratingDict[(u,i)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m itemsPerUser[u]]\n\u001b[1;32m      6\u001b[0m     userAverages[u] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(rs) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(rs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'itemsPerUser' is not defined"
     ]
    }
   ],
   "source": [
    "userAverages = {}\n",
    "itemAverages = {}\n",
    "\n",
    "for u in itemsPerUser:\n",
    "    rs = [ratingDict[(u,i)] for i in itemsPerUser[u]]\n",
    "    userAverages[u] = sum(rs) / len(rs)\n",
    "    \n",
    "for i in usersPerItem:\n",
    "    rs = [ratingDict[(u,i)] for u in usersPerItem[i]]\n",
    "    itemAverages[i] = sum(rs) / len(rs)\n",
    "\n",
    "def pearson(i1, i2):\n",
    "    # Between two items\n",
    "    iBar1 = itemAverages[i1]\n",
    "    iBar2 = itemAverages[i2]\n",
    "    inter = usersPerItem[i1].intersection(usersPerItem[i2])\n",
    "    numer = 0\n",
    "    denom1 = 0\n",
    "    denom2 = 0\n",
    "    for u in inter:\n",
    "        numer += (ratingDict[(u,i1)] - iBar1)*(ratingDict[(u,i2)] - iBar2)\n",
    "    for u in inter: #usersPerItem[i1]:\n",
    "        denom1 += (ratingDict[(u,i1)] - iBar1)**2\n",
    "    #for u in usersPerItem[i2]:\n",
    "        denom2 += (ratingDict[(u,i2)] - iBar2)**2\n",
    "    denom = math.sqrt(denom1) * math.sqrt(denom2)\n",
    "    if denom == 0: return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "jaccard_threshold = 0.10  \n",
    "cosine_threshold = 0.10  \n",
    "\n",
    "def predictPurchase(u,b):\n",
    "    maxJaccardSim = 0\n",
    "    maxCosineSim = 0\n",
    "    users = set(ratingsPerItem[b])\n",
    "    \n",
    "    # Compute similarities for all items rated by the user\n",
    "    for b2, _ in ratingsPerUser[u]:\n",
    "        # Jaccard similarity\n",
    "        jaccardSim = Jaccard(users, set(ratingsPerItem[b2]))\n",
    "        if jaccardSim > maxJaccardSim:\n",
    "            maxJaccardSim = jaccardSim\n",
    "        \n",
    "        # Cosine similarity\n",
    "        cosineSim = Cosine(b, b2)\n",
    "        if cosineSim > maxCosineSim:\n",
    "            maxCosineSim = cosineSim\n",
    "    \n",
    "    # Prediction logic\n",
    "    pred = 0\n",
    "    if (\n",
    "        maxJaccardSim > jaccard_threshold \n",
    "        or maxCosineSim > cosine_threshold \n",
    "        or (b in popular_items) \n",
    "        or len(ratingsPerItem[b]) > 40\n",
    "    ):\n",
    "        pred = 1\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Factor Model\n",
    "\n",
    "They aim to map users and items to a shared latent space where their interactions can be represented as the dot product of their latent representations. Latent factors represent categories that are present in the data. With a higher k, you have more specific categories. Whats going is we are trying to predict a user u’s rating of item i. Therefore, we look at P to find a vector representing user u, and their preferences or “affinity” toward all of the latent factors. Then, we look at Q to find a vector representing item i and it’s “affinity” toward all the latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = set()\n",
    "item_set = set()\n",
    "purchased_set = set()\n",
    "\n",
    "# Collect from train and validation data\n",
    "for u, b, r,_ in train_data + valid_data + test_data:\n",
    "    user_set.add(u)\n",
    "    item_set.add(b)\n",
    "    purchased_set.add((u, b))  # Add observed interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_IDs = {}\n",
    "item_IDs = {}\n",
    "interactions = []\n",
    "\n",
    "# Could adapt to any dataset, this one is from\n",
    "# https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home\n",
    "for u, i, r,_ in train_data + valid_data + test_data:\n",
    "    if not u in user_IDs: user_IDs[u] = len(user_IDs)\n",
    "    if not i in item_IDs: item_IDs[i] = len(item_IDs)\n",
    "    interactions.append((u,i,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1276840/1276840 [00:02<00:00, 429933.44it/s]\n"
     ]
    }
   ],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u,i,r,_ in tqdm(train_data + valid_data + test_data):\n",
    "    ratingsPerUser[u].append((i,r))\n",
    "    ratingsPerItem[i].append((u,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negatives(data, item_set, purchased_set, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Generate one negative sample for each positive interaction in the dataset.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of tuples (user, item, ...) representing positive interactions.\n",
    "        item_set (set): Set of all possible items.\n",
    "        purchased_set (set): Set of all (user, item) pairs that are positive interactions.\n",
    "        dataset_name (str): Name of the dataset for logging.\n",
    "\n",
    "    Returns:\n",
    "        list: List of negative samples corresponding to each positive interaction.\n",
    "    \"\"\"\n",
    "    negatives = []\n",
    "    for user, item, rating, time in tqdm(data, desc=f\"Creating negative set for {dataset_name}\"):\n",
    "        # Get items the user has not purchased\n",
    "        non_purchased_items = item_set - {i for i, r in ratingsPerUser[user]}\n",
    "        \n",
    "        # Randomly sample one non-purchased item\n",
    "        if non_purchased_items:\n",
    "            sampled_item = random.choice(list(non_purchased_items))\n",
    "            negatives.append((user, sampled_item, rating, time, 0))  # Label as 0 for negative sample\n",
    "    \n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating negative set for train_data: 100%|██████████| 984048/984048 [22:32<00:00, 727.74it/s]  \n",
      "Creating negative set for valid_data: 100%|██████████| 146396/146396 [04:19<00:00, 563.72it/s]\n",
      "Creating negative set for test_data: 100%|██████████| 146396/146396 [03:41<00:00, 662.05it/s] \n"
     ]
    }
   ],
   "source": [
    "# Generate negative samples for each dataset\n",
    "neg_train_data = generate_negatives(train_data, item_set, purchased_set, \"train_data\")\n",
    "neg_valid_data = generate_negatives(valid_data, item_set, purchased_set, \"valid_data\")\n",
    "neg_test_data = generate_negatives(test_data, item_set, purchased_set, \"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and negative samples for each dataset\n",
    "pos_train_data = [(u, i, r, t, 1) for u, i, r, t in train_data]\n",
    "train_data_full = pos_train_data + neg_train_data\n",
    "\n",
    "pos_valid_data = [(u, i, r, t, 1) for u, i, r, t in valid_data]\n",
    "valid_data_full = pos_valid_data + neg_valid_data\n",
    "\n",
    "pos_test_data = [(u, i, r, t, 1) for u, i, r, t in test_data]\n",
    "test_data_full = pos_test_data + neg_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put into csv\n",
    "train_data_full_df = pd.DataFrame(train_data_full, columns=[\"user_ID\", \"parent_asin\", \"rating\", \"timestamp\", \"label\"])\n",
    "train_data_full_df.to_csv(\"data/train_data_full.csv\", index=False)  # Set index=False to avoid writing row numbers\n",
    "\n",
    "valid_data_full_df = pd.DataFrame(valid_data_full, columns=[\"user_ID\", \"parent_asin\", \"rating\", \"timestamp\", \"label\"])\n",
    "valid_data_full_df.to_csv(\"data/valid_data_full.csv\", index=False)  # Set index=False to avoid writing row numbers\n",
    "\n",
    "test_data_full_df = pd.DataFrame(test_data_full, columns=[\"user_ID\", \"parent_asin\", \"rating\", \"timestamp\", \"label\"])\n",
    "test_data_full_df.to_csv(\"data/test_data_full.csv\", index=False)  # Set index=False to avoid writing row numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full_df = pd.read_csv(\"data/train_data_full.csv\")\n",
    "valid_data_full_df = pd.read_csv(\"data/valid_data_full.csv\")\n",
    "test_data_full_df = pd.read_csv(\"data/test_data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_ID</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>B00466BGS4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1284039832000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>B004074Y6U</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1365629921000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>B005ZKC4FO</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1365671303000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFZUK3MTBIBEDQOPAK3OATUOUKLA</td>\n",
       "      <td>B00FWRNW1A</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1382845563000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFZUK3MTBIBEDQOPAK3OATUOUKLA</td>\n",
       "      <td>B00EEDJHXA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1391875795000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_ID parent_asin  rating      timestamp  label\n",
       "0  AFSKPY37N3C43SOI5IEXEK5JSIYA  B00466BGS4     5.0  1284039832000      1\n",
       "1  AFSKPY37N3C43SOI5IEXEK5JSIYA  B004074Y6U     2.0  1365629921000      1\n",
       "2  AFSKPY37N3C43SOI5IEXEK5JSIYA  B005ZKC4FO     4.0  1365671303000      1\n",
       "3  AFZUK3MTBIBEDQOPAK3OATUOUKLA  B00FWRNW1A     5.0  1382845563000      1\n",
       "4  AFZUK3MTBIBEDQOPAK3OATUOUKLA  B00EEDJHXA     5.0  1391875795000      1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert user and item IDs to indices\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "train_data_full_df['user_idx'] = user_encoder.fit_transform(train_data_full_df['user_ID'])\n",
    "train_data_full_df['item_idx'] = item_encoder.fit_transform(train_data_full_df['parent_asin'])\n",
    "\n",
    "# Labels (1 for purchase, 0 for no purchase)\n",
    "labels = train_data_full_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_factors):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        self.user_factors = nn.Embedding(num_users, num_factors)  # User latent factors\n",
    "        self.item_factors = nn.Embedding(num_items, num_factors)  # Item latent factors\n",
    "        self.user_biases = nn.Embedding(num_users, 1)  # User biases\n",
    "        self.item_biases = nn.Embedding(num_items, 1)  # Item biases\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))  # Global bias\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        # Dot product of user and item latent factors\n",
    "        dot_product = (self.user_factors(user_idx) * self.item_factors(item_idx)).sum(dim=1)\n",
    "        # Add biases\n",
    "        prediction = dot_product + self.user_biases(user_idx).squeeze() + self.item_biases(item_idx).squeeze() + self.global_bias\n",
    "        return torch.sigmoid(prediction)  # Sigmoid for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 1/20 [00:00<00:11,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 2/20 [00:01<00:10,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 2.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 3/20 [00:01<00:10,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 2.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 4/20 [00:02<00:09,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 1.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 5/20 [00:02<00:08,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 1.9942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 6/20 [00:03<00:09,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 1.9893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 7/20 [00:04<00:09,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 1.9843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 8/20 [00:05<00:10,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 1.9794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 9/20 [00:06<00:09,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 1.9744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|█████     | 10/20 [00:08<00:10,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 1.9695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▌    | 11/20 [00:09<00:10,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 1.9645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████    | 12/20 [00:10<00:09,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 1.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▌   | 13/20 [00:11<00:08,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 1.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 14/20 [00:13<00:07,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 1.9499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|███████▌  | 15/20 [00:14<00:05,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 1.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████  | 16/20 [00:15<00:04,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 1.9402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████▌ | 17/20 [00:16<00:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 1.9352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 18/20 [00:16<00:02,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 1.9304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▌| 19/20 [00:17<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 1.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 20/20 [00:18<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 1.9208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_items = len(item_encoder.classes_)\n",
    "num_factors = 20\n",
    "\n",
    "model = LatentFactorModel(num_users, num_items, num_factors)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare training data\n",
    "user_idx = torch.tensor(train_data_full_df['user_idx'].values)\n",
    "item_idx = torch.tensor(train_data_full_df['item_idx'].values)\n",
    "labels = torch.tensor(train_data_full_df['label'].values, dtype=torch.float32)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(user_idx, item_idx)\n",
    "    loss = criterion(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5004\n"
     ]
    }
   ],
   "source": [
    "valid_data_full_df['user_idx'] = user_encoder.transform(valid_data_full_df['user_ID'])\n",
    "valid_data_full_df['item_idx'] = item_encoder.transform(valid_data_full_df['parent_asin'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    valid_user_idx = torch.tensor(valid_data_full_df['user_idx'].values)\n",
    "    valid_item_idx = torch.tensor(valid_data_full_df['item_idx'].values)\n",
    "    valid_labels = torch.tensor(valid_data_full_df['label'].values, dtype=torch.float32)\n",
    "    predictions = model(valid_user_idx, valid_item_idx)\n",
    "    predictions = (predictions > 0.5).float()\n",
    "    accuracy = (predictions == valid_labels).float().mean()\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing using Julian Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 984048/984048 [00:03<00:00, 317572.11it/s]\n"
     ]
    }
   ],
   "source": [
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u,i,r,_ in tqdm(train_data):\n",
    "    ratingsPerUser[u].append((i,r))\n",
    "    ratingsPerItem[i].append((u,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(train_data)\n",
    "nUsers = len(ratingsPerUser)\n",
    "nItems = len(ratingsPerItem)\n",
    "users = list(ratingsPerUser.keys())\n",
    "items = list(ratingsPerItem.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AFSKPY37N3C43SOI5IEXEK5JSIYA', 'B00466BGS4', 5.0, 1284039832000)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0345318521047755"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingMean = sum(r for _, _, r,_ in train_data) / len(train_data)\n",
    "ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ratingMean\n",
    "labels = [d[2] for d in train_data]\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "userGamma = {}\n",
    "itemGamma = {}\n",
    "K = 2\n",
    "\n",
    "for u in ratingsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "for i in ratingsPerItem:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    alpha = theta[index]\n",
    "    index += 1\n",
    "    userBiases = dict(zip(users, theta[index:index+nUsers]))\n",
    "    index += nUsers\n",
    "    itemBiases = dict(zip(items, theta[index:index+nItems]))\n",
    "    index += nItems\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index:index+K]\n",
    "        index += K\n",
    "    for i in items:\n",
    "        itemGamma[i] = theta[index:index+K]\n",
    "        index += K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(x, y):\n",
    "    return sum([a*b for a,b in zip(x,y)])\n",
    "\n",
    "def prediction(user, item):\n",
    "    user_bias = userBiases.get(user, 0.0)\n",
    "    item_bias = itemBiases.get(item, 0.0)\n",
    "    user_gamma = userGamma.get(user, [0.0] * K)\n",
    "    item_gamma = itemGamma.get(item, [0.0] * K)\n",
    "    \n",
    "    return alpha + user_bias + item_bias + inner(user_gamma, item_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d[0], d[1]) for d in train_data]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in users:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*userGamma[u][k]**2\n",
    "    for i in items:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*itemGamma[i][k]**2\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(train_data)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in ratingsPerUser:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in ratingsPerItem:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for d in train_data:\n",
    "        u,i = d[0], d[1]\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - d[2]\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2/N*itemGamma[i][k]*diff\n",
    "            dItemGamma[i][k] += 2/N*userGamma[u][k]*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        for k in range(K):\n",
    "            dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return numpy.array(dtheta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.7948607568590096\n",
      "MSE = 1.7825268761593958\n",
      "MSE = 4.937117337940384\n",
      "MSE = 1.7901026378080709\n",
      "MSE = 1.758678996833904\n",
      "MSE = 1.7559860318138723\n",
      "MSE = 1.746190872633137\n",
      "MSE = 1.7072620050547018\n",
      "MSE = 1.7058387808708848\n",
      "MSE = 1.7079095155464608\n",
      "MSE = 1.7085188643196114\n",
      "MSE = 1.7091300243197818\n",
      "MSE = 1.7097001460331234\n",
      "MSE = 1.7099404022262072\n",
      "MSE = 1.7100464782445248\n",
      "MSE = 1.7100736247394632\n",
      "MSE = 1.7100776752401614\n",
      "MSE = 1.7100756733588656\n",
      "MSE = 1.7100764681832057\n",
      "MSE = 1.710081923555373\n",
      "MSE = 1.7100850825657585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.96753697e+00, -9.64337059e-04, -1.79492485e-02, ...,\n",
       "         1.18062141e-06,  7.93900829e-07, -1.64548958e-06]),\n",
       " 1.7412779714577473,\n",
       " {'grad': array([-8.19462171e-07,  6.84519973e-10, -9.36256818e-09, ...,\n",
       "          2.35676681e-09,  1.58770706e-09, -3.29115959e-09]),\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 21,\n",
       "  'nit': 17,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + # Initialize alpha\n",
    "                                   [0.0]*(nUsers+nItems) + # Initialize beta\n",
    "                                   [random.random() * 0.1 - 0.05 for k in range(K*(nUsers+nItems))], # Gamma\n",
    "                             derivative, args = (labels, 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize parameters\n",
    "alpha = 0.0  # Initial global bias\n",
    "K = 2  # Number of latent factors\n",
    "\n",
    "# Extract all unique users and items from all datasets\n",
    "all_users = pd.concat([train_data_full_df['user_ID'], \n",
    "                       valid_data_full_df['user_ID'], \n",
    "                       test_data_full_df['user_ID']]).unique()\n",
    "all_items = pd.concat([train_data_full_df['parent_asin'], \n",
    "                       valid_data_full_df['parent_asin'], \n",
    "                       test_data_full_df['parent_asin']]).unique()\n",
    "\n",
    "# Initialize biases and latent factors for all users and items\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "userGamma = {u: [random.random() * 0.1 - 0.05 for _ in range(K)] for u in all_users}\n",
    "itemGamma = {i: [random.random() * 0.1 - 0.05 for _ in range(K)] for i in all_items}\n",
    "\n",
    "# Pre-group reviews by user and item\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for _, row in train_data_full_df.iterrows():\n",
    "    user = row['user_ID']\n",
    "    item = row['parent_asin']\n",
    "    label = row['label']\n",
    "    reviewsPerUser[user].append((item, label))\n",
    "    reviewsPerItem[item].append((user, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "# Sigmoid function to convert raw scores to probabilities\n",
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        z = math.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        z = math.exp(x)\n",
    "        return z / (1 + z)\n",
    "\n",
    "# Prediction function for classification\n",
    "def prediction(user, item):\n",
    "    # Retrieve biases and latent factors\n",
    "    user_bias = userBiases.get(user, 0.0)\n",
    "    item_bias = itemBiases.get(item, 0.0)\n",
    "    user_gamma = userGamma.get(user, [0.0] * K)\n",
    "    item_gamma = itemGamma.get(item, [0.0] * K)\n",
    "\n",
    "    # Calculate score using dot product\n",
    "    score = alpha + user_bias + item_bias + sum(ug * ig for ug, ig in zip(user_gamma, item_gamma))\n",
    "    return sigmoid(score)\n",
    "\n",
    "# Log-loss function for binary classification\n",
    "def log_loss(predictions, labels):\n",
    "    epsilon = 1e-10  # To avoid log(0)\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)  # Avoid extreme probabilities\n",
    "    labels = np.array(labels)\n",
    "    losses = -labels * np.log(predictions) - (1 - labels) * np.log(1 - predictions)\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Cost function for optimization\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [\n",
    "        sigmoid(alpha + userBiases[u] + itemBiases[i] + sum(ug * ig for ug, ig in zip(userGamma[u], itemGamma[i])))\n",
    "        for u, i, _ in train_data_full_df[['user_ID', 'parent_asin', 'label']].itertuples(index=False)\n",
    "    ]\n",
    "    labels = train_data_full_df['label'].tolist()\n",
    "    loss = log_loss(predictions, labels)\n",
    "\n",
    "    # Add regularization\n",
    "    for u in userBiases:\n",
    "        loss += lamb * userBiases[u] ** 2\n",
    "        for k in range(K):\n",
    "            loss += lamb * userGamma[u][k] ** 2\n",
    "    for i in itemBiases:\n",
    "        loss += lamb * itemBiases[i] ** 2\n",
    "        for k in range(K):\n",
    "            loss += lamb * itemGamma[i][k] ** 2\n",
    "    return loss\n",
    "\n",
    "# Optimized derivative function\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(train_data_full_df)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {u: [0.0 for _ in range(K)] for u in reviewsPerUser}\n",
    "    dItemGamma = {i: [0.0 for _ in range(K)] for i in reviewsPerItem}\n",
    "\n",
    "    # Compute derivatives using grouped reviews\n",
    "    for u, items in reviewsPerUser.items():\n",
    "        for item, label in items:\n",
    "            pred = prediction(u, item)\n",
    "            diff = pred - label\n",
    "            dalpha += diff\n",
    "            dUserBiases[u] += diff\n",
    "            dItemBiases[item] += diff\n",
    "            for k in range(K):\n",
    "                dUserGamma[u][k] += itemGamma[item][k] * diff\n",
    "                dItemGamma[item][k] += userGamma[u][k] * diff\n",
    "\n",
    "    # Apply regularization\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2 * lamb * userBiases[u]\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2 * lamb * userGamma[u][k]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2 * lamb * itemBiases[i]\n",
    "        for k in range(K):\n",
    "            dItemGamma[i][k] += 2 * lamb * itemGamma[i][k]\n",
    "\n",
    "    dtheta = [dalpha / N] + [dUserBiases[u] / N for u in all_users] + \\\n",
    "             [dItemBiases[i] / N for i in all_items]\n",
    "    for u in all_users:\n",
    "        dtheta += [g / N for g in dUserGamma[u]]\n",
    "    for i in all_items:\n",
    "        dtheta += [g / N for g in dItemGamma[i]]\n",
    "\n",
    "    return np.array(dtheta)\n",
    "\n",
    "# Evaluate classification performance\n",
    "def evaluate_classification(data_df):\n",
    "    labels = data_df['label'].tolist()\n",
    "    predictions = [prediction(row['user_ID'], row['parent_asin']) for _, row in data_df.iterrows()]\n",
    "    binary_predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(labels, binary_predictions)\n",
    "    precision = precision_score(labels, binary_predictions)\n",
    "    recall = recall_score(labels, binary_predictions)\n",
    "    auc_roc = roc_auc_score(labels, predictions)  # Use raw probabilities for AUC\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "    return accuracy, precision, recall, auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'B0BGQLZD65'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Optimize with scipy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train_data_full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_users\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_items\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_users\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_items\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mderivative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate on validation data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m evaluate_classification(valid_data_full_df)\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:199\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    187\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[1;32m    188\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m: iprint,\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: maxls}\n\u001b[0;32m--> 199\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    202\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    203\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuncalls\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    204\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    205\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarnflag\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m    206\u001b[0m f \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:309\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    306\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# _prepare_scalar_function can use bounds=None to represent no bounds\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    315\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/scipy/optimize/_optimize.py:402\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    398\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/recommender/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[144], line 41\u001b[0m, in \u001b[0;36mcost\u001b[0;34m(theta, labels, lamb)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost\u001b[39m(theta, labels, lamb):\n\u001b[1;32m     40\u001b[0m     unpack(theta)\n\u001b[0;32m---> 41\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     42\u001b[0m         sigmoid(alpha \u001b[38;5;241m+\u001b[39m userBiases[u] \u001b[38;5;241m+\u001b[39m itemBiases[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(ug \u001b[38;5;241m*\u001b[39m ig \u001b[38;5;28;01mfor\u001b[39;00m ug, ig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(userGamma[u], itemGamma[i])))\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m u, i, _ \u001b[38;5;129;01min\u001b[39;00m train_data_full_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_asin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m     ]\n\u001b[1;32m     45\u001b[0m     labels \u001b[38;5;241m=\u001b[39m train_data_full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m log_loss(predictions, labels)\n",
      "Cell \u001b[0;32mIn[144], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost\u001b[39m(theta, labels, lamb):\n\u001b[1;32m     40\u001b[0m     unpack(theta)\n\u001b[1;32m     41\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 42\u001b[0m         sigmoid(alpha \u001b[38;5;241m+\u001b[39m userBiases[u] \u001b[38;5;241m+\u001b[39m \u001b[43mitemBiases\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(ug \u001b[38;5;241m*\u001b[39m ig \u001b[38;5;28;01mfor\u001b[39;00m ug, ig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(userGamma[u], itemGamma[i])))\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m u, i, _ \u001b[38;5;129;01min\u001b[39;00m train_data_full_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_asin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m     ]\n\u001b[1;32m     45\u001b[0m     labels \u001b[38;5;241m=\u001b[39m train_data_full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m log_loss(predictions, labels)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'B0BGQLZD65'"
     ]
    }
   ],
   "source": [
    "# Optimize with scipy\n",
    "train_labels = train_data_full_df['label'].tolist()\n",
    "scipy.optimize.fmin_l_bfgs_b(\n",
    "    cost,\n",
    "    [alpha] + [0.0] * (len(all_users) + len(all_items)) + \n",
    "    [random.random() * 0.1 - 0.05 for _ in range(K * (len(all_users) + len(all_items)))],\n",
    "    derivative,\n",
    "    args=(train_labels, 0.001)\n",
    ")\n",
    "\n",
    "# Evaluate on validation data\n",
    "evaluate_classification(valid_data_full_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
